# OpenAlpha_Evolve: Contribute to Improve this Project

![openalpha_evolve_workflow](https://github.com/user-attachments/assets/9d4709ad-0072-44ae-bbb5-7eea1c5fa08c)

OpenAlpha_Evolve is an open-source Python framework inspired by the groundbreaking research on autonomous coding agents like DeepMind's AlphaEvolve. It's a **regeneration** of the core idea: an intelligent system that iteratively writes, tests, and improves code using Large Language Models (LLMs) via LiteLLM, guided by the principles of evolution.

Our mission is to provide an accessible, understandable, and extensible platform for researchers, developers, and enthusiasts to explore the fascinating intersection of AI, code generation, and automated problem-solving.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE.md)

## Table of Contents
- [‚ú® The Vision: AI-Driven Algorithmic Innovation](#-the-vision-ai-driven-algorithmic-innovation)
- [üß† How It Works: The Evolutionary Cycle](#-how-it-works-the-evolutionary-cycle)
- [üöÄ Key Features](#-key-features)
- [üìÇ Project Structure](#-project-structure)
- [üèÅ Getting Started](#-getting-started)
- [üí° Defining Your Own Algorithmic Quests!](#-defining-your-own-algorithmic-quests)
- [üîÆ The Horizon: Future Evolution](#-the-horizon-future-evolution)
- [ü§ù Join the Evolution: Contributing](#-join-the-evolution-contributing)
- [üìú License](#-license)
- [üôè Homage](#-homage)

---
![image](https://github.com/user-attachments/assets/ff498bb7-5608-46ca-9357-fd9b55b76800)
![image](https://github.com/user-attachments/assets/c1b4184a-f5d5-43fd-8f50-3e729c104e11)



## ‚ú® The Vision: AI-Driven Algorithmic Innovation

Imagine an agent that can:

*   Understand a complex problem description.
*   Generate initial algorithmic solutions.
*   Rigorously test its own code.
*   Learn from failures and successes.
*   Evolve increasingly sophisticated and efficient algorithms over time.

OpenAlpha_Evolve is a step towards this vision. It's not just about generating code; it's about creating a system that *discovers* and *refines* solutions autonomously.

---
<img width="1253" alt="Screenshot 2025-05-19 at 12 17 58‚ÄØAM" src="https://github.com/user-attachments/assets/43d7c5a8-f361-438c-ac38-39717f28ee1f" />

## üß† How It Works: The Evolutionary Cycle

OpenAlpha_Evolve employs a modular, agent-based architecture to orchestrate an evolutionary process:

1.  **Task Definition**: You, the user, define the algorithmic "quest" ‚Äì the problem to be solved, including examples of inputs and expected outputs.
2.  **Prompt Engineering (`PromptDesignerAgent`)**: This agent crafts intelligent prompts for the LLM. It designs:
    *   *Initial Prompts*: To generate the first set of candidate solutions.
    *   *Mutation Prompts*: To introduce variations and improvements to existing solutions, often requesting changes in a "diff" format.
    *   *Bug-Fix Prompts*: To guide the LLM in correcting errors from previous attempts, also typically expecting a "diff".
3.  **Code Generation (`CodeGeneratorAgent`)**: ÈªòËÆ§ÈÄöËøá Azure OpenAI GPT-5-miniÔºàManaged Identity + `azure_api.AzureOpenAIClient`ÔºâÁîüÊàê‰ª£Á†ÅÔºåÂπ∂Âú® `USE_AZURE_OPENAI=False` Êó∂ÂõûÈÄÄÂà∞ LiteLLM/ÂÖ∂‰ªñÊèê‰æõÂïÜ„ÄÇËã•ÊèêÁ§∫Ë¶ÅÊ±ÇËæìÂá∫ diffÔºå‰ª£ÁêÜ‰ºöÂ∞ùËØïÂ∞Ü diff Â∫îÁî®Âà∞Áà∂‰ª£‰ª£Á†Å„ÄÇ
4.  **Evaluation (`EvaluatorAgent`)**: The generated code is put to the test!
    *   *Syntax Check*: Is the code valid Python?
    *   *Execution*: The code is run in a temporary, isolated environment against the input/output examples defined in the task.
    *   *Fitness Scoring*: Programs are scored based on correctness (how many test cases pass), efficiency (runtime), and other potential metrics.
5.  **Database (`DatabaseAgent`)**: All programs (code, fitness scores, generation, lineage) are stored, creating a record of the evolutionary history (currently in-memory).
6.  **Selection (`SelectionControllerAgent`)**: The "survival of the fittest" principle in action. This agent selects:
    *   *Parents*: Promising programs from the current generation to produce offspring.
    *   *Survivors*: The best programs from both the current population and new offspring to advance to the next generation.
7.  **Iteration**: This cycle repeats for a defined number of generations, with each new generation aiming to produce better solutions than the last.
8.  **Orchestration (`TaskManagerAgent`)**: The maestro of the operation, coordinating all other agents and managing the overall evolutionary loop.

---

## üöÄ Key Features

*   **LLM-Powered Code Generation**: ÂºÄÁÆ±Âç≥Áî®Âú∞ÂØπÊé• Azure OpenAI GPT-5ÔºàÂê´Ëá™ÂÆö‰πâË∂ÖÊó∂/ÈáçËØï„ÄÅÊâòÁÆ°Ë∫´‰ªΩËÆ§ËØÅÔºâÔºåÂπ∂ÂèØÈÄöËøá LiteLLM ÂõûÈÄÄÂà∞‰ªªÊÑèÂÖ∂‰ªñÊ®°ÂûãÊèê‰æõÂïÜ„ÄÇ
*   **Evolutionary Algorithm Core**: Implements iterative improvement through selection, LLM-driven mutation/bug-fixing using diffs, and survival.
*   **Modular Agent Architecture**: Easily extend or replace individual components (e.g., use a different LLM, database, or evaluation strategy).
*   **Automated Program Evaluation**: Syntax checking and functional testing against user-provided examples. Code execution is sandboxed using **Docker containers** for improved security and dependency management, with configurable timeout mechanisms.
*   **ËøûÁª≠Â§öÊåáÊ†á + ÂêØÂèëÂºèÊêúÁ¥¢Ê®°Âºè**: `TaskDefinition` Áé∞Â∑≤ÊîØÊåÅ `evaluation_mode: "metrics"`ÔºåÁî®Êà∑ÂèØÊåÇÊé•Ëá™ÂÆö‰πâ `evaluate_candidate()` Ê®°ÂùóËøîÂõû `dict[str, float]`ÔºåÂØπ‚ÄúÊêúÁ¥¢Âô®‚ÄùÁ±ªÁ®ãÂ∫èËøõË°åËøûÁª≠„ÄÅÂ§öÁª¥ÊâìÂàÜÔºà‰∏ªÊåáÊ†á + Á∫øÊÄßÊ†áÈáèÂåñ + success ‰ºòÂÖàÁ∫ßÔºâÔºåSelectionController ‰ºöÊçÆÊ≠§ÊéíÂ∫è„ÄÇ
*   **Configuration Management**: Easily tweak parameters like population size, number of generations, LLM models, API settings, and Docker configurations via `config/settings.py` and `.env`.
*   **Detailed Logging**: Comprehensive logs provide insights into each step of the evolutionary process.
*   **Diff-based Mutations**: The system is designed to use diffs for mutations and bug fixes, allowing for more targeted code modifications by the LLM.
*   **Prompt Â§öÊ†∑Âåñ**: `PromptDesignerAgent` Âú®ÂàùÂßã/ÂèòÂºÇ/‰øÆÂ§çÈò∂ÊÆµÈÉΩ‰ºöÈöèÊú∫ÈááÊ†∑Â§öÁßçÊ®°ÊùøÔºàËÆ∞ÂΩï `prompt_id`ÔºâÔºå‰æø‰∫éÂêéÁª≠ÁªüËÆ°ÊàñÊâ©Â±ï ‚Äúmeta-prompt evolution‚Äù„ÄÇ
*   **Open Source & Extensible**: Built with Python, designed for experimentation and community contributions.

---

## üìÇ Project Structure

```text
./
‚îú‚îÄ‚îÄ code_generator/      # Agent responsible for generating code using LLMs.
‚îú‚îÄ‚îÄ database_agent/      # Agent for managing the storage and retrieval of programs and their metadata.
‚îú‚îÄ‚îÄ evaluator_agent/     # Agent that evaluates the generated code for syntax, execution, and fitness.
‚îú‚îÄ‚îÄ prompt_designer/     # Agent that crafts prompts for the LLM for initial generation, mutation, and bug fixing.
‚îú‚îÄ‚îÄ selection_controller/  # Agent that implements the selection strategy for parent and survivor programs.
‚îú‚îÄ‚îÄ task_manager/        # Agent that orchestrates the overall evolutionary loop and coordinates other agents.
‚îú‚îÄ‚îÄ config/                  # Holds configuration files, primarily `settings.py` for system parameters and API keys.
‚îú‚îÄ‚îÄ core/                    # Defines core data structures and interfaces, like `Program` and `TaskDefinition`.
‚îú‚îÄ‚îÄ tests/                   # Includes unit and integration tests to ensure code quality and correctness.
‚îú‚îÄ‚îÄ main.py                  # The main entry point to run the OpenAlpha_Evolve system and start an evolutionary run.
‚îú‚îÄ‚îÄ requirements.txt         # Lists all Python package dependencies required to run the project.
‚îú‚îÄ‚îÄ .env.example             # An example file showing the environment variables needed, such as API keys. Copy this to `.env` and fill in your values.
‚îú‚îÄ‚îÄ .gitignore               # Specifies intentionally untracked files that Git should ignore (e.g., `.env`, `__pycache__/`).
‚îú‚îÄ‚îÄ LICENSE.md               # Contains the full text of the MIT License under which the project is distributed.
‚îî‚îÄ‚îÄ README.md                # This file! Provides an overview of the project, setup instructions, and documentation.
```

---

## üèÅ Getting Started

1.  **Prerequisites**:
    *   Python 3.10+
    *   `pip` for package management
    *   `git` for cloning
    *   **Docker**: For sandboxed code evaluation. Ensure Docker Desktop (Windows/Mac) or Docker Engine (Linux) is installed and running. Visit [docker.com](https://www.docker.com/get-started) for installation instructions.

2.  **Clone the Repository**:
    ```bash
    git clone https://github.com/shyamsaktawat/OpenAlpha_Evolve.git
    cd OpenAlpha_Evolve
    ```

3.  **Set Up a Virtual Environment** (recommended):
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

4.  **Install Dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

5.  **Set Up Environment Variables (ÂøÖÈúÄ)**:
    *   ‰ªìÂ∫ìÂΩìÂâçÊú™ÈôÑÂ∏¶ `.env_example`ÔºåËØ∑ÊâãÂä®ÂàõÂª∫ `.env`ÔºàÊàñÂú® shell ‰∏≠Áõ¥Êé•ÂØºÂá∫ÂèòÈáèÔºâ„ÄÇ
    *   **Azure OpenAIÔºàÈªòËÆ§Ë∑ØÂæÑÔºâ**Ôºö
        ```bash
        USE_AZURE_OPENAI=True
        ENDPOINT_URL="https://your-endpoint.openai.azure.com/"
        DEPLOYMENT_NAME="gpt-5-mini"
        AZURE_MANAGED_IDENTITY_CLIENT_ID="your-managed-identity-client-id"
        AZURE_API_VERSION="2025-01-01-preview"
        AZURE_HTTP_TIMEOUT_SECONDS=600
        AZURE_MAX_RETRIES=3
        ```
        ÈúÄÁ°Æ‰øùËøêË°åÁéØÂ¢ÉÂÖ∑Â§áÂèØÁî®ÁöÑ Azure Managed IdentityÔºåÊàñÂú®Êú¨Âú∞ËÆæÁΩÆ `DefaultAzureCredential` ÊîØÊåÅÁöÑÂá≠ÊçÆ„ÄÇ
    *   **LiteLLM / ÂÖ∂‰ªñÊ®°ÂûãÔºàÂèØÈÄâÂõûÈÄÄÔºâ**ÔºöÂΩì `USE_AZURE_OPENAI=False` Êó∂Ôºå`CodeGeneratorAgent` ‰ΩøÁî® LiteLLM ÁöÑ `LITELLM_DEFAULT_MODEL`„ÄÇËØ∑Ê†πÊçÆ [LiteLLM ÊñáÊ°£](https://docs.litellm.ai/docs/providers) ÈÖçÁΩÆÁõ∏Â∫î API KeyÔºå‰æãÂ¶ÇÔºö
        ```bash
        USE_AZURE_OPENAI=False
        LITELLM_DEFAULT_MODEL="gpt-4o-mini"
        OPENAI_API_KEY="sk-..."
        ```
    *   ÂèØÊ†πÊçÆÈúÄË¶ÅÊ∑ªÂä† Google/Anthropic/Cohere Á≠âÈ¢ùÂ§ñÂèòÈáèÔºåÊàñ‰∏∫ËØÑ‰º∞Âô®ËÆæÁΩÆ `EVALUATION_API_KEY`„ÄÇ

6.  **Run OpenAlpha_Evolve!**
    ‰ª• Dijkstra Á§∫‰æãÂêØÂä®Ôºö
    ```bash
    python main.py examples/shortest_path.yaml
    ```
    Watch the logs in your terminal to see the evolutionary process unfold! Log files are also saved to `alpha_evolve.log` (by default).

7.  **Launch the Gradio Web Interface**
    Interact with the system via the web UI. To start the Gradio app:
    ```bash
    python app.py
    ```
    Gradio will display a local URL (e.g., http://127.0.0.1:7860) and a public share link if enabled. Open this in your browser to define custom tasks and run the evolution process interactively.

### üå≤ Continuous Metrics Demo: Erd≈ës 993

The repository now includes a fully metrics-driven benchmark inspired by AlphaEvolve's Erd≈ës‚ÄìMoser explorations. It treats the program under evolution as a **search heuristic** instead of a direct solver.

```bash
python -m main examples/erdos_993.yaml
```

Key facts:

- `evaluation_mode: "metrics"` tells the framework to import `examples.erdos_993_eval.evaluate_candidate`.
- Your candidate must implement `search_erdos_993(seed: int, budget: int) -> dict`, iterate over random seeds, and return telemetry such as `candidate`, `best_violation`, and `samples_tried`.
- The evaluator computes continuous metrics (`score`, `valid_ratio`, `runtime_penalty`, ‚Ä¶) and SelectionController scalarizes them (primary key `score`, fallback linear weights).
- Even if no counter-example is found yet, the best violation magnitude provides a gradient for evolution, mirroring the ‚Äúsoft objectives‚Äù used in the original AlphaEvolve work.

---

## üí° Defining Your Own Algorithmic Quests!

Want to challenge OpenAlpha_Evolve with a new problem? It's easy! You can define your tasks in two ways:

### 1. Using YAML Files (Recommended)

Create a YAML file in the `examples` directory with the following structure:

```yaml
task_id: "your_task_id"
task_description: |
  Your detailed problem description here.
  Be specific about function names, expected behavior, and constraints.
function_name: "your_function_name"
allowed_imports: ["module1", "module2"]

tests:
  - description: "Test group description" # Describes a group of related tests
    name: "Test group name" # A name for this test group
    test_cases: # This should be a list of individual test cases
      - input: [arg1, arg2]  # First test case
        output: expected_output # Expected result for this input
        # Each test case uses either 'output' for direct comparison
        # or 'validation_func' for more complex validation.
      - input: [arg_for_validation_func_1, arg_for_validation_func_2] # Second test case
        validation_func: |
          def validate(output_from_function):
              # Custom validation logic for this specific test case's output
              # For example, check if output is within a certain range,
              # or if it has specific properties.
              return isinstance(output_from_function, bool) and output_from_function is True
```

See the example in `examples/shortest_path.yaml`

#### Metrics Mode & Search Heuristics

For research-style problems (new combinatorial bounds, hardware heuristics, etc.) you can switch the task into metrics mode:

```yaml
evaluation_mode: "metrics"
metrics_eval_module: "examples.erdos_993_eval"
metrics_primary_key: "score"
metrics_scalarization:
  score: 1.0
  runtime_penalty: 0.1
metrics_config:
  seeds: [0, 1, 2, 3, 4]
  budget: 512
```

Guidelines:

- Implement a *searcher* function (e.g., `search_erdos_993(seed: int, budget: int)`) that can continue exploration across generations. It should return telemetry such as `{"candidate": ..., "best_violation": 0.42, "samples_tried": 1024}`.
- Provide an evaluation module exposing `evaluate_candidate(program_module, task_definition) -> dict[str, float]`. This keeps heavy validation logic (simulators, proof checkers, etc.) outside of Docker test harnesses.
- Define a primary metric and optional scalarization weights so SelectionController can order individuals in multi-dimensional metric space.
- Check `examples/erdos_993.yaml` + `examples/erdos_993_eval.py` for a complete reference implementation.

### 2. Using Python Code (Legacy)

You can still define tasks programmatically using the `TaskDefinition` class:

```python
from core.task_definition import TaskDefinition

task = TaskDefinition(
    id="your_task_id",
    description="Your detailed problem description",
    function_name_to_evolve="your_function_name",
    input_output_examples=[
        {"input": [arg1, arg2], "output": expected_output},
        # More examples...
    ],
    allowed_imports=["module1", "module2"]
)
```

### Best Practices for Task Definition

Crafting effective task definitions is key to guiding OpenAlpha_Evolve successfully. Consider these tips:

*   **Be Clear and Unambiguous**: Write task descriptions as if you're explaining the problem to another developer. Avoid jargon where possible, or explain it clearly.
*   **Provide Diverse and Comprehensive Examples**: Your test cases are the primary way the agent verifies its generated code.
    *   Include typical use cases
    *   Cover edge cases (empty inputs, boundary values, etc.)
    *   Include examples that test different logical paths
    *   Use validation functions for complex checks
*   **Start Simple, Then Increase Complexity**: Break down complex problems into simpler versions first.
*   **Specify Constraints and Edge Cases**: Mention specific constraints and edge cases in the description.
*   **Define Expected Function Signature**: Clearly state the expected function name and parameters.
*   **Iterate and Refine**: Review and refine your task definition based on the agent's performance.

---

## üîÆ The Horizon: Future Evolution



---

## ü§ù Join the Evolution: Contributing

This is an open invitation to collaborate! Whether you're an AI researcher, a Python developer, or simply an enthusiast, your contributions are welcome.

*   **Report Bugs**: Find an issue? Please create an issue on GitHub!
*   **Suggest Features**: Have an idea to make OpenAlpha_Evolve better? Open an issue to discuss it!
*   **Submit Pull Requests**:
    *   Fork the repository.
    *   Create a new branch for your feature or bugfix (`git checkout -b feature/your-feature-name`).
    *   Write clean, well-documented code.
    *   Add tests for your changes if applicable.
    *   Ensure your changes don't break existing functionality.
    *   Submit a pull request with a clear description of your changes!

Let's evolve this agent together!

---

## üìú License

This project is licensed under the **MIT License**. See the `LICENSE.md` file for details.

---

## üôè Homage

OpenAlpha_Evolve is proudly inspired by the pioneering work of the Google DeepMind team on AlphaEvolve and other related research in LLM-driven code generation and automated discovery. This project aims to make the core concepts more accessible for broader experimentation and learning. We stand on the shoulders of giants.

---

*Disclaimer: This is an experimental project. Generated code may not always be optimal, correct, or secure. Always review and test code thoroughly, especially before using it in production environments.* 
